{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "ad0b5ab2-f059-4004-9040-570dc2491602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccd9c69b-f873-4d23-b4aa-208eb0605213",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Work:\n",
    "    def __init__(self, i, t, c, l):\n",
    "        self.title = t\n",
    "        self.content = c\n",
    "        self.labels = l\n",
    "        self.id = i\n",
    "    def __str__(self):\n",
    "        return f\"id: \\\"{self.id}\\\"\\ntitle: \\\"{self.title}\\\"\\ncontent: \\\"{self.content}\\\"\\nlabels: {self.labels}\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72d4916f-f0a6-4580-bab0-f8ec187039bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    data=[]\n",
    "    with open(path, 'r', encoding='utf-8') as reader:\n",
    "        for line in reader:\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c16d2e-a3f0-4cf0-9da1-b299799cf023",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_csv(path):\n",
    "    data=[]\n",
    "    with open(path, 'r', encoding='utf-8') as file:\n",
    "        rows = csv.reader(file)\n",
    "        for row in rows:\n",
    "            label = {\"id\": row[0], \"name\": row[1]}\n",
    "            data.append(label)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13379698-cc83-4c75-a96a-dd9d31518b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_str(datas):\n",
    "    string = \"\"\n",
    "    for data in datas:\n",
    "        t_str = data['body']\n",
    "        t_str = t_str.replace(u'\\u3000', u'')\n",
    "        string += t_str\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "006edaf9-3eda-4846-8152-a2ab5986b779",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_work(data):\n",
    "    if data['labels'] != None:\n",
    "        t_labels = create_label_vector(labels, data['labels'])\n",
    "    else:\n",
    "        t_labels = create_label_vector(labels, [\"\"])\n",
    "    \n",
    "    w = Work(data['id'], data['metadata']['title'], add_str(data['content'])[:512], t_labels)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83827d4e-115d-4ae9-8236-ee0e2d539759",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_label_vector(total_labels, target_labels):\n",
    "    return_label = []\n",
    "    for i, label in enumerate(total_labels):\n",
    "        for t_label in target_labels:\n",
    "            if label['name'] == t_label:\n",
    "                return_label.append(label['id'])\n",
    "    return return_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1b72f345-6a8a-42fe-be57-65c7c3fd3eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'id': '11', 'name': 'TS'}, {'id': '12', 'name': 'スキル'}, {'id': '13', 'name': '夫婦'}, {'id': '14', 'name': 'ステータス'}, {'id': '15', 'name': '学生'}, {'id': '16', 'name': '後輩'}, {'id': '17', 'name': '少年'}, {'id': '18', 'name': '社会人'}, {'id': '18', 'name': 'サラリーマン'}, {'id': '19', 'name': '大学生'}]\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "labels = load_csv(\"label_list_cleaned.csv\")\n",
    "print(labels[15:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "deb2df84-d83f-4f5c-9dc0-6bfb1c25dfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117735405488A-512.jsonl original lines: 9585\n",
      "117735405488A-512.jsonl cleaned lines: 9236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 9236/9236 [00:04<00:00, 1998.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117735405488B-512.jsonl original lines: 6480\n",
      "117735405488B-512.jsonl cleaned lines: 6253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6253/6253 [00:03<00:00, 1844.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117735405488C-512.jsonl original lines: 8156\n",
      "117735405488C-512.jsonl cleaned lines: 7898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7898/7898 [00:03<00:00, 2069.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117735405489A-512.jsonl original lines: 7418\n",
      "117735405489A-512.jsonl cleaned lines: 7168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7168/7168 [00:04<00:00, 1518.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117735405489B-512.jsonl original lines: 6471\n",
      "117735405489B-512.jsonl cleaned lines: 6322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6322/6322 [00:03<00:00, 1738.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11773540549-512.jsonl original lines: 7539\n",
      "11773540549-512.jsonl cleaned lines: 7254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 7254/7254 [00:04<00:00, 1809.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1177354055-512.jsonl original lines: 2958\n",
      "1177354055-512.jsonl cleaned lines: 2861\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 2861/2861 [00:01<00:00, 2081.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681641041-512.jsonl original lines: 466\n",
      "1681641041-512.jsonl cleaned lines: 447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 447/447 [00:00<00:00, 1652.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681645221-512.jsonl original lines: 6774\n",
      "1681645221-512.jsonl cleaned lines: 6607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6607/6607 [00:02<00:00, 2294.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681645222-512.jsonl original lines: 3886\n",
      "1681645222-512.jsonl cleaned lines: 3749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 3749/3749 [00:01<00:00, 2148.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681670018-512.jsonl original lines: 9\n",
      "1681670018-512.jsonl cleaned lines: 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 9/9 [00:00<00:00, 1800.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681670042-512.jsonl original lines: 9254\n",
      "1681670042-512.jsonl cleaned lines: 8837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 8837/8837 [00:02<00:00, 2957.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681692761-512.jsonl original lines: 12\n",
      "1681692761-512.jsonl cleaned lines: 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 12/12 [00:00<00:00, 3998.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681692785-512.jsonl original lines: 1788\n",
      "1681692785-512.jsonl cleaned lines: 1744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1744/1744 [00:00<00:00, 4725.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1681692786-512.jsonl original lines: 171\n",
      "1681692786-512.jsonl cleaned lines: 168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 168/168 [00:00<00:00, 6998.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4852201425-512.jsonl original lines: 1087\n",
      "4852201425-512.jsonl cleaned lines: 1062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1062/1062 [00:00<00:00, 1358.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 88.04787516593933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "from os import walk\n",
    "\n",
    "works = []\n",
    "st_time = time.time()\n",
    "filenames = next(walk(\"testdata\"),  (None, None, []))[2]\n",
    "for filename in filenames:\n",
    "    datas = load_jsonl(f\"testdata/{filename}\")\n",
    "    \n",
    "    print(f\"{filename} original lines: {len(datas)}\")\n",
    "    for data in datas:\n",
    "        if data['labels'] == None:\n",
    "            datas.remove(data)\n",
    "    print(f\"{filename} cleaned lines: {len(datas)}\")\n",
    "    \n",
    "    for data in tqdm.tqdm(datas):\n",
    "        w = create_work(data)\n",
    "        work = {\"id\": w.id, \"title\": w.title, \"content\": w.content, \"labels\": w.labels}\n",
    "        works.append(work)\n",
    "        \n",
    "print(f\"Time: {time.time()-st_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d97a13db-a51c-4929-8cd1-be5f0d5c83dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████████████████████████████████████████████████████████████████▌    | 57713/61372 [00:37<00:02, 1548.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57713\n",
      "{'id': '1177354054880200602', 'title': '歴史のレシピ', 'content': '宇宙人は何故地球を侵略しないのか？宇宙人は何故人類を滅ぼさないのか？宇宙戦争は何故、起こらないのか？それらを真剣に考えた時、彼女があの時言っていた事こそが、この世の真実なのではないか、と、私は思った。あれは、私が……否、俺が高3の夏。合宿も兼ねた天体観測の計画を話し合う放課後の部室での出来事だった。「だから！宇宙人なんて何処にもいないんです」そう叫んだ彼女は、大きなくりくりの目を更に見開き、ぷくりと頬を膨らませた。「そもそも宇宙には、地球人以外の知的生命体なんて存在しないんですよ」そう言いながら呆れたように俺から視線を反らした彼女は、「皆が宇宙人だって思ってるモノは、ぜーんぶ地球人の子孫なんです。私の言ってる事、分かります？」と言って、俺の方をキッと睨み付けた。真っ黒な瞳とツヤツヤに輝く天使の輪を携えたショートボブの黒髪の彼女は、少し着崩した夏服のブラウスに1年の象徴である緑色の細いリボンを結んでいる。2つも年下の女の子に、俺はもう30分近くも『宇宙人は地球人の子孫である』と言う彼女の持論を説かれているのだ。マジ、参る。何でこんな時に限って誰も来ないんだよ。もう部活、始まる時間だろ？『', 'labels': ['49', '115', '145']}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(len(works))\n",
    "for work in tqdm.tqdm(works):\n",
    "    if work['labels'] == []:\n",
    "        works.remove(work)\n",
    "print(len(works))\n",
    "print(works[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e79f74f-cab1-42e5-b0b9-52903a545458",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 69627/69627 [01:20<00:00, 862.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:01:20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "wakati = MeCab.Tagger(\"-Owakati\")\n",
    "st_time = time.time()\n",
    "for work in tqdm.tqdm(works):\n",
    "    content = work['content']\n",
    "    work['wakati'] = \"\"\n",
    "    tokens = wakati.parse(content).split()\n",
    "    work['wakati'] += \" \".join(tokens) + \" \"\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-st_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4afd7705-0558-4a23-8d7b-a3887f573bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55701\n",
      "6963\n",
      "6963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, vt = train_test_split(works, test_size=0.2)\n",
    "valid, test = train_test_split(vt, test_size=0.5)\n",
    "print(len(train))\n",
    "print(len(valid))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1cc4b33-11a3-4f9e-a4d9-b08aa865d6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n"
     ]
    }
   ],
   "source": [
    "unsortIds = [label['id'] for label in labels]\n",
    "ids = []\n",
    "for id in unsortIds:\n",
    "    if id not in ids:\n",
    "        ids.append(id)\n",
    "print(len(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bbe55ff9-ea9a-4d47-b71d-fa2f69663c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x-> test:55701 valid:6963 test:6963\n",
      "y-> test:55701 valid:6963 test:6963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "x_train = [work['wakati'] for work in train]\n",
    "x_valid = [work['wakati'] for work in valid]\n",
    "x_test  = [work['wakati'] for work in test ]\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=(ids))\n",
    "y_train = mlb.fit_transform([work['labels'] for work in train])\n",
    "y_valid = mlb.fit_transform([work['labels'] for work in valid])\n",
    "y_test  = mlb.fit_transform([work['labels'] for work in test ])\n",
    "\n",
    "print(\"x-> test:{0} valid:{1} test:{2}\".format(len(x_train), len(x_valid), len(x_test)))\n",
    "print(\"y-> test:{0} valid:{1} test:{2}\".format(len(y_train), len(y_valid), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "019baead-0f6e-4409-bf27-930bdb42ac4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:03:34\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "### MODELS ###\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "st_time = time.time()\n",
    "classifier = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    #('clf', OneVsRestClassifier(LinearSVC(), n_jobs=4))\n",
    "    #('clf', OneVsRestClassifier(SVC(kernel='linear')))\n",
    "    ('clf', OneVsRestClassifier(estimator= LogisticRegression(solver='liblinear')))\n",
    "])\n",
    "classifier.fit(x_train, y_train)\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-st_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6796925-ea58-4ceb-9d09-18648eb5a0b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "threshold = 0.05\n",
    "\n",
    "### VALIDATION ###\n",
    "st_time = time.time()\n",
    "predicted = classifier.predict_proba(x_valid)\n",
    "predicted = predicted>threshold\n",
    "#valid_labels = mlb.inverse_transform(predicted)\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-st_time)))\n",
    "print(classification_report(y_valid, predicted, target_names=ids))\n",
    "### END OF VALIDATION ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cee873f-f2aa-48b4-bfe1-5ffa57d0ed8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### TEST ###\n",
    "st_time = time.time()\n",
    "predicted = classifier.predict_proba(x_test)\n",
    "predicted = predicted>threshold\n",
    "#valid_labels = mlb.inverse_transform(predicted)\n",
    "print(time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-st_time)))\n",
    "print(classification_report(y_test, predicted, target_names=ids))\n",
    "### END OF TEST ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "64029e0f-a4ba-4951-9646-fa702818aaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAVE MODEL ###\n",
    "import pickle\n",
    "\n",
    "# save\n",
    "with open('front512_LR.pkl','wb') as f:\n",
    "    pickle.dump(classifier, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c4c5d-6172-460f-8487-4a5ef60b0c2d",
   "metadata": {},
   "source": [
    "### BERTの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b604881d-6eef-44c8-9369-7c741227892a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q transformers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "66fe432a-ae0d-4878-a90a-018ca50d4b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4bc284fe-8327-48bd-add8-53ec21480809",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68048dce2a244ef9beecdad77200b90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/1.95k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d3643a35ceb425bab5d8096d9aca9e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset sem_eval2018_task1/subtask5.english (download: 5.70 MiB, generated: 1.24 MiB, post-processed: Unknown size, total: 6.94 MiB) to C:\\Users\\User\\.cache\\huggingface\\datasets\\sem_eval2018_task1\\subtask5.english\\1.1.0\\a7c0de8b805f1988b118882fb289ccfbbeb9085c7820b6f046b5887e234af182...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65309b88632e4446abe988f704914c59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3e8f011e47e409f8678fb5318891266",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.98M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e87a23ee61644c5497bde1887d6e73ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/6838 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3259 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/886 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sem_eval2018_task1 downloaded and prepared to C:\\Users\\User\\.cache\\huggingface\\datasets\\sem_eval2018_task1\\subtask5.english\\1.1.0\\a7c0de8b805f1988b118882fb289ccfbbeb9085c7820b6f046b5887e234af182. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1c172b98e7e49e4926306e7391f2ce2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"sem_eval_2018_task_1\", \"subtask5.english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "b51dc539-ecba-4e59-9af9-3970cca70013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ID': '2017-En-21441',\n",
       " 'Tweet': \"“Worry is a down payment on a problem you may never have'. \\xa0Joyce Meyer.  #motivation #leadership #worry\",\n",
       " 'anger': False,\n",
       " 'anticipation': True,\n",
       " 'disgust': False,\n",
       " 'fear': False,\n",
       " 'joy': False,\n",
       " 'love': False,\n",
       " 'optimism': True,\n",
       " 'pessimism': False,\n",
       " 'sadness': False,\n",
       " 'surprise': False,\n",
       " 'trust': True}"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "74b8356a-a3b6-4faa-afb3-4449d188073a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', mecab_kwargs={\"mecab_dic\": \"unidic\", \"mecab_option\": None})\n",
    "mlb = MultiLabelBinarizer(classes=(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "9d9aa671-e3c3-4b8e-a608-9d5f7203c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(work):\n",
    "    text = work['content']\n",
    "    encoding = tokenizer.encode(text, max_length=512, truncation=True, return_tensors='pt')[0]\n",
    "    labels_batch =  mlb.fit_transform(work['labels'])\n",
    "    labels_matrix = np.zeros((len(text), len(ids)))\n",
    "    for idx, label in enuerate(ids):\n",
    "        labels_matrix[:, idx] = labels_batch[label]\n",
    "    encoding['labels'] = labels_matrix.tolist()\n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "6c1e1254-0cdd-4b2c-9bcc-6cc1e9869192",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1292/2130335003.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoded_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mworks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "encoded_dataset = works.map(preprocess_data, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ce64fd0e-bc0d-4e0c-a55f-9f37df60c9e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1177354054880199356</td>\n",
       "      <td>彼女は頭の上にミカンを乗せていた。ミカンセイ空間にようこそ</td>\n",
       "      <td>web小説ですし、出来るだけくだけた書き方でいきます。この作品はノンセンス(荒唐無稽な物事を...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1177354054880199370</td>\n",
       "      <td>ノー・イヤー・ヒーロー</td>\n",
       "      <td>常識が変わる瞬間を見たことがあるか？例えばクラハムベルがはじめて遠距離通信を行った瞬間とか、...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1177354054880199563</td>\n",
       "      <td>星の代わりに</td>\n",
       "      <td>今は午後一時半。お腹に少しは物を入れている。タバコもいっぷくすませたし、あとにすることは限ら...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1177354054880199764</td>\n",
       "      <td>長耳のベアラー</td>\n",
       "      <td>必要とされる場所には自然と人は集まってくる。人が集まれば活気が出来上がり、夜でも賑やかな街に...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1177354054880199944</td>\n",
       "      <td>海の底へ</td>\n",
       "      <td>「玲さんは、なんでこんなところまで来たの？」拓真に聞かれる。「…だから、拓真に会いにだよ。」...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                          title  \\\n",
       "0  1177354054880199356  彼女は頭の上にミカンを乗せていた。ミカンセイ空間にようこそ   \n",
       "1  1177354054880199370                    ノー・イヤー・ヒーロー   \n",
       "2  1177354054880199563                         星の代わりに   \n",
       "3  1177354054880199764                        長耳のベアラー   \n",
       "4  1177354054880199944                           海の底へ   \n",
       "\n",
       "                                             content  \\\n",
       "0  web小説ですし、出来るだけくだけた書き方でいきます。この作品はノンセンス(荒唐無稽な物事を...   \n",
       "1  常識が変わる瞬間を見たことがあるか？例えばクラハムベルがはじめて遠距離通信を行った瞬間とか、...   \n",
       "2  今は午後一時半。お腹に少しは物を入れている。タバコもいっぷくすませたし、あとにすることは限ら...   \n",
       "3  必要とされる場所には自然と人は集まってくる。人が集まれば活気が出来上がり、夜でも賑やかな街に...   \n",
       "4  「玲さんは、なんでこんなところまで来たの？」拓真に聞かれる。「…だから、拓真に会いにだよ。」...   \n",
       "\n",
       "                                              labels  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "#print(works[0]['content'])\n",
    "#print(works[0]['labels'])\n",
    "works_id = [work['id'] for work in works]\n",
    "works_title = [work['title'] for work in works]\n",
    "works_content = [work['content'] for work in works]\n",
    "\n",
    "mlb = MultiLabelBinarizer(classes=(ids))\n",
    "works_labels = [work['labels'] for work in works]\n",
    "works_labels = mlb.fit_transform(works_labels)\n",
    "#print(works_labels.tolist())\n",
    "works_df = pd.DataFrame({'id': works_id, 'title': works_title, 'content': works_content, 'labels': works_labels.tolist()})\n",
    "works_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c3cab727-d4b5-42eb-aa81-722e75bc8c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb\n",
    "#https://qiita.com/sugulu_Ogawa_ISID/items/697bd03499c1de9cf082\n",
    "#https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb#scrollTo=0enAb0W9o25W\n",
    "\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking', mecab_kwargs={\"mecab_dic\": \"unidic\", \"mecab_option\": None})\n",
    "\n",
    "def tokenizer_512(input_text):\n",
    "    return tokenizer.encode(input_text, max_length=512, truncation=True, return_tensors='pt')[0]\n",
    "\n",
    "#TEXT = torchtext.legacy.data.Field(sequential=True, tokenize=tokenizer_512, use_vocab=False, lower=False, include_lengths=False, batch_first=True, fix_length=max_length, pad_token=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "fdb0ec08-6bf3-4806-84af-056a8d83060b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "69627it [03:04, 378.07it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>labels</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1177354054880199356</td>\n",
       "      <td>彼女は頭の上にミカンを乗せていた。ミカンセイ空間にようこそ</td>\n",
       "      <td>web小説ですし、出来るだけくだけた書き方でいきます。この作品はノンセンス(荒唐無稽な物事を...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[tensor(2), tensor(28047), tensor(1527), tenso...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1177354054880199370</td>\n",
       "      <td>ノー・イヤー・ヒーロー</td>\n",
       "      <td>常識が変わる瞬間を見たことがあるか？例えばクラハムベルがはじめて遠距離通信を行った瞬間とか、...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[tensor(2), tensor(17773), tensor(14), tensor(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1177354054880199563</td>\n",
       "      <td>星の代わりに</td>\n",
       "      <td>今は午後一時半。お腹に少しは物を入れている。タバコもいっぷくすませたし、あとにすることは限ら...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[tensor(2), tensor(744), tensor(9), tensor(477...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1177354054880199764</td>\n",
       "      <td>長耳のベアラー</td>\n",
       "      <td>必要とされる場所には自然と人は集まってくる。人が集まれば活気が出来上がり、夜でも賑やかな街に...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[tensor(2), tensor(727), tensor(13), tensor(26...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1177354054880199944</td>\n",
       "      <td>海の底へ</td>\n",
       "      <td>「玲さんは、なんでこんなところまで来たの？」拓真に聞かれる。「…だから、拓真に会いにだよ。」...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[tensor(2), tensor(36), tensor(16391), tensor(...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id                          title  \\\n",
       "0  1177354054880199356  彼女は頭の上にミカンを乗せていた。ミカンセイ空間にようこそ   \n",
       "1  1177354054880199370                    ノー・イヤー・ヒーロー   \n",
       "2  1177354054880199563                         星の代わりに   \n",
       "3  1177354054880199764                        長耳のベアラー   \n",
       "4  1177354054880199944                           海の底へ   \n",
       "\n",
       "                                             content  \\\n",
       "0  web小説ですし、出来るだけくだけた書き方でいきます。この作品はノンセンス(荒唐無稽な物事を...   \n",
       "1  常識が変わる瞬間を見たことがあるか？例えばクラハムベルがはじめて遠距離通信を行った瞬間とか、...   \n",
       "2  今は午後一時半。お腹に少しは物を入れている。タバコもいっぷくすませたし、あとにすることは限ら...   \n",
       "3  必要とされる場所には自然と人は集まってくる。人が集まれば活気が出来上がり、夜でも賑やかな街に...   \n",
       "4  「玲さんは、なんでこんなところまで来たの？」拓真に聞かれる。「…だから、拓真に会いにだよ。」...   \n",
       "\n",
       "                                              labels  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [tensor(2), tensor(28047), tensor(1527), tenso...  \n",
       "1  [tensor(2), tensor(17773), tensor(14), tensor(...  \n",
       "2  [tensor(2), tensor(744), tensor(9), tensor(477...  \n",
       "3  [tensor(2), tensor(727), tensor(13), tensor(26...  \n",
       "4  [tensor(2), tensor(36), tensor(16391), tensor(...  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = []\n",
    "for index, row in tqdm.tqdm(works_df.iterrows()):\n",
    "    tokens.append(tokenizer_512(row['content']))\n",
    "works_df['tokens'] = tokens\n",
    "works_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "cbc9de90-a17a-4e73-8ff2-398128b47e3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1292/2132046341.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mencoded_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mworks_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mworks_df\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mworks_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_format\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"torch\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "encoded_dataset = works_df.map(works_df, batched=True)\n",
    "works_df.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e61f4a40-68a7-44e6-b84a-8d0c508a70d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55701\n",
      "6963\n",
      "6963\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, vt = train_test_split(works_df, test_size=0.2)\n",
    "valid, test = train_test_split(vt, test_size=0.5)\n",
    "print(len(train))\n",
    "print(len(valid))\n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "dd0f8ec8-76f6-4f17-9538-e5e44df099f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55701 55701\n"
     ]
    }
   ],
   "source": [
    "train_x = train.loc[:, 'tokens']\n",
    "train_y = train.loc[:, 'labels']\n",
    "eval_x = valid.loc[:, 'tokens']\n",
    "eval_y = valid.loc[:, 'labels']\n",
    "print(len(train_x), len(train_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0b137160-af6a-483c-bb65-40cf4faffe38",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16  # BERTでは16、32あたりを使用する\n",
    "\n",
    "# 辞書オブジェクトにまとめる\n",
    "dataloaders_dict = {\"train\": train_x, \"val\": eval_x}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29157a6-8068-400b-a3e0-cd115e4004ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "# BERTの日本語学習済みパラメータのモデルです\n",
    "model = BertModel.from_pretrained('cl-tohoku/bert-base-japanese-whole-word-masking')\n",
    "#print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8ba34748-c9ef-4ca0-9bf0-0d20c7d158eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class BERT(nn.Module):\n",
    "    '''BERTモデルにLivedoorニュースの9クラスを判定する部分をつなげたモデル'''\n",
    "\n",
    "    def __init__(self):\n",
    "        super(BERT, self).__init__()\n",
    "\n",
    "        # BERTモジュール\n",
    "        self.bert = model  # 日本語学習済みのBERTモデル\n",
    "\n",
    "        # headにクラス予測を追加\n",
    "        # 入力はBERTの出力特徴量の次元768、出力は9クラス\n",
    "        self.cls = nn.Linear(in_features=768, out_features=9)\n",
    "\n",
    "        # 重み初期化処理\n",
    "        nn.init.normal_(self.cls.weight, std=0.02)\n",
    "        nn.init.normal_(self.cls.bias, 0)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        '''\n",
    "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
    "        '''\n",
    "\n",
    "        # BERTの基本モデル部分の順伝搬\n",
    "        # 順伝搬させる\n",
    "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
    "\n",
    "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
    "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
    "        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素\n",
    "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n",
    "        output = self.cls(vec_0)  # 全結合層\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "49e080b9-bc34-456d-a4cf-c3506277dc3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ネットワーク設定完了\n"
     ]
    }
   ],
   "source": [
    "# モデル構築\n",
    "net = BERT()\n",
    "\n",
    "# 訓練モードに設定\n",
    "net.train()\n",
    "\n",
    "print('ネットワーク設定完了')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "64c4673f-12e7-446a-8edb-0b8498b2b9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
    "\n",
    "# 1. まず全部を、勾配計算Falseにしてしまう\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
    "for param in net.bert.encoder.layer[-1].parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# 3. 識別器を勾配計算ありに変更\n",
    "for param in net.cls.parameters():\n",
    "    param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "dff91ccc-7795-437e-8d77-c7068eefc193",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# BERTの元の部分はファインチューニング\n",
    "optimizer = optim.Adam([\n",
    "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
    "    {'params': net.cls.parameters(), 'lr': 1e-4}\n",
    "])\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "343126e7-7518-484b-aef4-09db50d4344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
    "\n",
    "    # GPUが使えるかを確認\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"使用デバイス：\", device)\n",
    "    print('-----start-------')\n",
    "\n",
    "    # ネットワークをGPUへ\n",
    "    net.to(device)\n",
    "\n",
    "    # ネットワークがある程度固定であれば、高速化させる\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "    # ミニバッチのサイズ\n",
    "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
    "\n",
    "    # epochのループ\n",
    "    for epoch in range(num_epochs):\n",
    "        # epochごとの訓練と検証のループ\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                net.train()  # モデルを訓練モードに\n",
    "            else:\n",
    "                net.eval()   # モデルを検証モードに\n",
    "\n",
    "            epoch_loss = 0.0  # epochの損失和\n",
    "            epoch_corrects = 0  # epochの正解数\n",
    "            iteration = 1\n",
    "\n",
    "            # データローダーからミニバッチを取り出すループ\n",
    "            for batch in (dataloaders_dict[phase]):\n",
    "                # batchはTextとLableの辞書型変数\n",
    "\n",
    "                # GPUが使えるならGPUにデータを送る\n",
    "                inputs = batch.Text[0].to(device)  # 文章\n",
    "                labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "                # optimizerを初期化\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # 順伝搬（forward）計算\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "\n",
    "                    # BERTに入力\n",
    "                    outputs = net(inputs)\n",
    "\n",
    "                    loss = criterion(outputs, labels)  # 損失を計算\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "\n",
    "                    # 訓練時はバックプロパゲーション\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
    "                            acc = (torch.sum(preds == labels.data)).double()/batch_size\n",
    "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(iteration, loss.item(),  acc))\n",
    "\n",
    "                    iteration += 1\n",
    "\n",
    "                    # 損失と正解数の合計を更新\n",
    "                    epoch_loss += loss.item() * batch_size\n",
    "                    epoch_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            # epochごとのlossと正解率\n",
    "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
    "            epoch_acc = epoch_corrects.double() / len(dataloaders_dict[phase].dataset)\n",
    "\n",
    "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs, phase, epoch_loss, epoch_acc))\n",
    "\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7ce68866-d111-4c50-834c-bd7545e8ba02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用デバイス： cpu\n",
      "-----start-------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1292/1132204440.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mnet_trained\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1292/144888663.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(net, dataloaders_dict, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# ミニバッチのサイズ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataloaders_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"train\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;31m# epochのループ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5485\u001b[0m         ):\n\u001b[0;32m   5486\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5487\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5489\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Series' object has no attribute 'batch_size'"
     ]
    }
   ],
   "source": [
    "num_epochs = 5\n",
    "net_trained = train_model(net, dataloaders_dict, criterion, optimizer, num_epochs=num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "3016b824-7cf2-4633-ba07-10cfd44117ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net_trained' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_1292/1470770390.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cuda:0\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"cpu\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mnet_trained\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m# モデルを検証モードに\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mnet_trained\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# GPUが使えるならGPUへ送る\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'net_trained' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "net_trained.eval()   # モデルを検証モードに\n",
    "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
    "\n",
    "# epochの正解数を記録する変数\n",
    "epoch_corrects = 0\n",
    "\n",
    "for batch in tqdm(dl_test):  # testデータのDataLoader\n",
    "    # batchはTextとLableの辞書オブジェクト\n",
    "    # GPUが使えるならGPUにデータを送る\n",
    "    inputs = batch.Text[0].to(device)  # 文章\n",
    "    labels = batch.Label.to(device)  # ラベル\n",
    "\n",
    "    # 順伝搬（forward）計算\n",
    "    with torch.set_grad_enabled(False):\n",
    "\n",
    "        # BertForLivedoorに入力\n",
    "        outputs = net_trained(inputs)\n",
    "\n",
    "        loss = criterion(outputs, labels)  # 損失を計算\n",
    "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
    "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
    "\n",
    "# 正解率\n",
    "epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
    "\n",
    "print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47904397-643c-4f1d-9fe7-791872282659",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
